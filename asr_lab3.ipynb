{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5970bd3f-7b36-46bc-bcb6-82c1f9ff36bc",
   "metadata": {},
   "source": [
    "# Акустические модели\n",
    "\n",
    "Акустическая модель - это часть системы автоматического распознавания речи, которая используется для преобразования аудиосигнала речи в последовательность фонем или других единиц речевого звука. Акустическая модель обучается на большом наборе речевых данных, чтобы определить, какие звуки соответствуют конкретным акустическим признакам в аудиосигнале. Эта модель может использоваться вместе с другими компонентами, такими как языковая модель и модель декодирования, чтобы достичь более точного распознавания речи.\n",
    "\n",
    "В данной работе мы сконцентрируемся на обучении нейросетевых акустических моделей с помощью библиотек torch и torchaudio. Для экспериментов будем использовать базу [TIMIT](https://catalog.ldc.upenn.edu/LDC93s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from typing import List, Dict, Union, Set, Any\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d611e67-aec1-4714-a722-865ec980d83d",
   "metadata": {},
   "source": [
    "# Загрузка датасета TIMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e498e1d-a102-42a3-bf67-7389d89bb483",
   "metadata": {},
   "source": [
    "Официальная страница датасета TIMIT \n",
    "\n",
    "Для простоты загрузки данных удобнее всего пользоваться копией датасета, выложенной на kaggle \n",
    "\n",
    "https://www.kaggle.com/datasets/mfekadu/darpa-timit-acousticphonetic-continuous-speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb80729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Kaggle/kaggle-api - Docs kaggle \n",
    "# Simplest way: go to https://www.kaggle.com/settings , \"Create new token\" and move it into \"~/.kaggle\"\n",
    "\n",
    "#!kaggle datasets download -d mfekadu/darpa-timit-acousticphonetic-continuous-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ded70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip -o -q darpa-timit-acousticphonetic-continuous-speech.zip -d timit/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed5129-a418-463b-881b-8e0d10ac4ba7",
   "metadata": {},
   "source": [
    "# 1. Подготовка данных для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f0891-77c1-4e55-a60d-89d5890796bc",
   "metadata": {},
   "source": [
    "TIMIT является одной из самых широко используемых баз данных для изучения систем автоматического распознавания речи. База данных TIMIT содержит произнесения предложений различными дикторами. Каждое произнесение сопровождается его словной и фонетической разметкой.\n",
    "\n",
    "Для обучения акустической модели нам в первую очередь интересна фонетическая разметка произнесений. Такая разметка сопоставляет фонемы, которые были произнесены диктором, с временными интервалами в записи. Такая разметка позволит нам обучить пофреймовый классификатор, который будет предсказывать сказанную фонему."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df01d01-7bde-4236-8ce0-4983ba3b18e0",
   "metadata": {},
   "source": [
    "## 1.a. Загрузка базы с диска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimitDataset(Dataset):\n",
    "    \"\"\"Загрузка TIMIT данных с диска\"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.uri2wav = {}\n",
    "        self.uri2text = {}\n",
    "        self.uri2word_ali = {}\n",
    "        self.uri2phone_ali = {}\n",
    "        for d, _, fs in os.walk(data_path):\n",
    "            for f in fs:\n",
    "                full_path = f'{d}/{f}'\n",
    "                if f.endswith('.WAV'):\n",
    "                    # skip it. Use .wav instead\n",
    "                    pass\n",
    "                elif f.endswith('.wav'):\n",
    "                    stem = Path(f[:-4]).stem # .WAV.wav\n",
    "                    self.uri2wav[f'{d}/{stem}'] = full_path\n",
    "                elif f.endswith('.TXT'):\n",
    "                    stem = Path(f).stem\n",
    "                    self.uri2text[f'{d}/{stem}'] = full_path\n",
    "                elif f.endswith('.WRD'):\n",
    "                    stem = Path(f).stem\n",
    "                    self.uri2word_ali[f'{d}/{stem}'] = full_path\n",
    "                elif f.endswith('.PHN'):\n",
    "                    stem = Path(f).stem\n",
    "                    self.uri2phone_ali[f'{d}/{stem}'] = full_path\n",
    "                else:\n",
    "                    warnings.warn(f\"Unknown file type {full_path} . Skip it.\")\n",
    "        \n",
    "        self.uris = list(sorted(set(self.uri2wav.keys()) \\\n",
    "                                & set(self.uri2text.keys()) \\\n",
    "                                & set(self.uri2word_ali.keys()) \\\n",
    "                                &  set(self.uri2phone_ali.keys())\n",
    "                               ))\n",
    "        print(f\"Found {len(self.uris)} utterances in {self.data_path}. \", \n",
    "              f\"{len(self.uri2wav)} wavs, \", \n",
    "              f\"{len(self.uri2text)} texts, \",\n",
    "              f\"{len(self.uri2word_ali)} word alinments, \",\n",
    "             f\"{len(self.uri2phone_ali)} phone alignments\")\n",
    "    \n",
    "    def get_uri(self, index_or_uri: Union[str, int]):\n",
    "        if isinstance(index_or_uri, str):\n",
    "            uri = index_or_uri\n",
    "        else:\n",
    "            uri = self.uris[index_or_uri]\n",
    "        return uri\n",
    "    \n",
    "    \n",
    "    def get_audio(self, index_or_uri: Union[str, int]):\n",
    "        uri = self.get_uri(index_or_uri)\n",
    "        wav_path = self.uri2wav[uri]\n",
    "        wav_channels, sr = torchaudio.load(wav_path)\n",
    "        return wav_channels[0], sr \n",
    "        \n",
    "    def get_text(self, index_or_uri: Union[str, int]):\n",
    "        \"\"\" Return (start_sample, stop_sample, text)\"\"\"\n",
    "        uri = self.get_uri(index_or_uri)\n",
    "        txt_path = self.uri2text[uri]\n",
    "        with open(txt_path) as f:\n",
    "            start, stop, text = f.read().strip().split(maxsplit=2)\n",
    "            start, stop = int(start), int(stop)\n",
    "            assert start == 0, f\"{txt_path}\"\n",
    "        return start, stop, text\n",
    "    \n",
    "    def get_word_ali(self, index_or_uri):\n",
    "        \"\"\" Return [(start_sample, stop_sample, word), ...]\"\"\"\n",
    "        uri = self.get_uri(index_or_uri)\n",
    "        wrd_path = self.uri2word_ali[uri]\n",
    "        with open(wrd_path) as f:\n",
    "            words = [(int(start), int(stop), word) for start, stop, word in map(str.split, f.readlines())]\n",
    "        return words\n",
    "    \n",
    "    def get_phone_ali(self, index_or_uri):\n",
    "        \"\"\" Return [(start_sample, stop_sample, phone), ...]\"\"\"\n",
    "        uri = self.get_uri(index_or_uri)\n",
    "        ph_path = self.uri2phone_ali[uri]\n",
    "        with open(ph_path) as f:\n",
    "            phonemes = [(int(start), int(stop), ph) for start, stop, ph in map(str.split, f.readlines())]\n",
    "        return phonemes\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {\"uri\": self.get_uri(index),\n",
    "                \"audio\": self.get_audio(index),\n",
    "                \"text\": self.get_text(index),\n",
    "                \"word_ali\": self.get_word_ali(index),\n",
    "                \"phone_ali\": self.get_phone_ali(index)}       \n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO\n",
    "        # верните количество элементов в выборке \n",
    "        pass\n",
    "\n",
    "    def total_audio_samples(self) -> int:\n",
    "        #TODO\n",
    "        # верните суммарное количество отсчетов во всем аудио\n",
    "        pass\n",
    "\n",
    "    def total_num_words(self) -> int:\n",
    "        #TODO\n",
    "        # верните суммарное количество слов в словном выравнивании\n",
    "        pass\n",
    "    \n",
    "    def total_num_phones(self) -> int:\n",
    "        #TODO\n",
    "        # верните суммарное количество фонем в фонемном выравнивании\n",
    "        pass\n",
    "    \n",
    "    def get_vocab(self) -> Set[str]:\n",
    "        #TODO\n",
    "        # верните словарь, состоящий из уникальных слов из выборки \n",
    "        pass\n",
    "\n",
    "    def get_phones(self) -> Set[str]:\n",
    "        #TODO\n",
    "        # верните уникальный набор фонем, которые используются в выравнивании\n",
    "        pass\n",
    "\n",
    "    def phones_prior(self) -> Dict[str, float]:\n",
    "        #TODO\n",
    "        # верните мапинг фонемы в их априорные вероятности. \n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4195f3-a0a6-4e13-83a2-6f96f0e99ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_timit_dataset_stats():\n",
    "    test_ds = TimitDataset('timit/data/TEST/')\n",
    "\n",
    "    print(\"Len\")\n",
    "    assert len(test_ds) == 1680, f\"{len(test_ds)}\"\n",
    "\n",
    "    print(\"Audio\")\n",
    "    audio_len = test_ds.total_audio_samples()\n",
    "    assert audio_len == 82986452, f\"{audio_len}\"\n",
    "\n",
    "    print(\"Words\")\n",
    "    words_len = test_ds.total_num_words()\n",
    "    assert words_len == 14553, f\"{words_len}\"\n",
    "\n",
    "    print(\"Phones\")\n",
    "    phones_len = test_ds.total_num_phones()\n",
    "    assert phones_len == 64145, f\"{phones_len}\"\n",
    "\n",
    "    print(\"Vocab\")\n",
    "    vocab = test_ds.get_vocab()\n",
    "    assert len(set(vocab)) == 2378, f\"{len(set(vocab))}\"\n",
    "\n",
    "    print(\"Phones vocab\")\n",
    "    phones = test_ds.get_phones()\n",
    "    assert len(set(phones)) == 61, f\"{len(set(phones))}\"\n",
    "    \n",
    "    print(\"Phones prior\")\n",
    "    priors = test_ds.phones_prior()\n",
    "    assert np.isclose(sum(priors.values()), 1.0), f\"sum(priors.values())\"\n",
    "    pmin, pmax = min(priors.keys(), key=priors.get), max(priors.keys(), key=priors.get)\n",
    "    assert pmin == 'eng', pmin\n",
    "    assert pmax == 'h#', pmax\n",
    "    print(\"Test 1.a passed\")\n",
    "test_timit_dataset_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eaa226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ds = TimitDataset('timit/data/TEST/')\n",
    "item = test_ds[5]\n",
    "print(item['uri'])\n",
    "print(item['text'][2])\n",
    "display.display(display.Audio(item['audio'][0].numpy(), rate=item['audio'][1]))\n",
    "print('---words---')\n",
    "for start, stop, word in item['word_ali']:\n",
    "    print(word)\n",
    "    display.display(display.Audio(item['audio'][0][start:stop].numpy(), rate=item['audio'][1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e16fa85-78d8-4604-a55d-377b197ec001",
   "metadata": {},
   "source": [
    "## 1.b. Экстрактор фич\n",
    "Для того чтобы построить акустическую модель, первым делом надо извлечь признаки аудио сигнала. Для распознавания речь принято использовать fbank признаки. fbank/MelSpectrogram признаки получается из амплитудного спектра сигнала путем свертки спекта с треугольными фильтрами в мел-шкале. Есть множество реализаций данных признаков в различных библиотеках (kaldi, librosa, torchaudio) и все они имеют свои особенности. В данной работе мы будем использовать реализацию из библиотеки torchaudio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e8d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate=16000,\n",
    "        n_fft=400,\n",
    "        hop_length=160,\n",
    "        n_mels=40,\n",
    "        f_max=7600,\n",
    "        spec_aug_max_fmask=80,\n",
    "        spec_aug_max_tmask=80,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        #TODO\n",
    "        # инициализируйте обработчик fbank фич из torchaudio\n",
    "        # self.mel_spec = ...\n",
    "        \n",
    "        \n",
    "    def samples2frames(self, num_samples: int) -> int:\n",
    "        # TODO\n",
    "        # Верните количество кадров в спектрограмме, соответствующей вавке длиной num_samples\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def feats_dim(self):\n",
    "        # TODO\n",
    "        # Верните количество извлекаемых фич\n",
    "        pass \n",
    "    \n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        mel = self.mel_spec(waveform)\n",
    "        return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d861c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_samples2frames():\n",
    "    fe = FeatureExtractor()\n",
    "    for i in tqdm(range(15000, 40000)):\n",
    "        wav = torch.zeros(i)\n",
    "        feats = fe(wav)\n",
    "        assert feats.shape[-2] == fe.feats_dim, f\"{i} {feats.shape[-2]=}, {fe.feats_dim}\"\n",
    "\n",
    "        assert feats.shape[-1] == fe.samples2frames(i), f\"{i} {feats.shape[-1]=}, {fe.samples2frames(i)}\"\n",
    "        \n",
    "    print('Test 1.b passed')\n",
    "test_samples2frames()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c53730-3c9e-495e-bf5d-dd258d879841",
   "metadata": {},
   "source": [
    "## 1.с. Таргеты и объединение данных в батчи \n",
    "\n",
    "Акустическая Модель (АМ) - пофреймовый классификатор, который предсказывает фонему для каждого кадра аудио. Для обучения AM будем использовать фонемное выравнивание. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd07a3ef-8dd3-4411-96fe-42d597a8d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TimitDataset('timit/data/TRAIN/')\n",
    "print(train_ds[0])\n",
    "\n",
    "# Строим мапинг из написания фонемы в ее id \n",
    "phones = train_ds.get_phones() \n",
    "phones.remove('pau')\n",
    "phones.remove('epi')\n",
    "phones.remove('h#')\n",
    "\n",
    "# Фонемы паузы должны иметь индекс 0\n",
    "PHONE2ID = {p:i for i, p in enumerate(['pau'] + list(sorted(phones)))}\n",
    "PHONE2ID['epi'] = 0\n",
    "PHONE2ID['h#'] = 0\n",
    "print(PHONE2ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatsPhoneDataset(TimitDataset):\n",
    "    def __init__(self, data_path, feature_extractor: FeatureExtractor, phone2id):\n",
    "        super().__init__(data_path)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.phone2id = phone2id\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        orig_item = super().__getitem__(index)\n",
    "        wav, sr = orig_item['audio']\n",
    "        assert sr == self.feature_extractor.sample_rate, f\"wrong sr for {index}\"\n",
    "        # подготавливаем пофреймовые фичи\n",
    "        feats = self.feature_extractor(wav)\n",
    "        feats = feats.squeeze(dim=0).transpose(0, 1) # time x feats\n",
    "\n",
    "        # создаем пофреймовое выравнивание \n",
    "        targets = torch.zeros(feats.shape[0], dtype=torch.long)\n",
    "        # TODO \n",
    "        # заполните пофреймовое фонемное выравнивание targets idшниками фонем\n",
    "        # используйте phone_ali \n",
    "        pass \n",
    "        \n",
    "        return {\"uri\": orig_item[\"uri\"],\n",
    "                \"feats\": feats,\n",
    "                \"targets\": targets, \n",
    "                \"src_key_padding_mask\": torch.zeros(feats.shape[0], dtype=torch.bool)}\n",
    "    \n",
    "    def collate_pad(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Функция объединения элементов в один батч\"\"\"\n",
    "        # TODO \n",
    "        # Реализуйте функцию, которая объединяет несколько item'ов датасета в один батч\n",
    "        # See collate_fn https://pytorch.org/docs/stable/data.html\n",
    "        # Входные данные и маску надо вернуть таком формате, в каком работает с данными torch.nn.Transformer\n",
    "        # targets надо склеить тензор с одной осью. Длина оси будет равна суммарному количеству кадров в батче\n",
    "        pass\n",
    "        \n",
    "        return {'feats': feats, # (Time, Batch, feats)\n",
    "               'targets': targets, #(SumTime)\n",
    "               'src_key_padding_mask': src_key_padding_mask, #(Batch, Time)\n",
    "               }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_collate_pad():\n",
    "    fe = FeatureExtractor()\n",
    "    test_ds = FeatsPhoneDataset('timit/data/TEST/', feature_extractor=fe, phone2id=PHONE2ID)\n",
    "\n",
    "    for i in range(20):\n",
    "        targets = test_ds[i]['targets']\n",
    "        orig_ph_ali = test_ds.get_phone_ali(i)\n",
    "        targets_set = set(targets.tolist())\n",
    "        orig_set = set([PHONE2ID[ph] for *_, ph in orig_ph_ali])\n",
    "        assert targets_set == orig_set, f\"{i} \\n{targets_set} \\n {orig_set} \\n {orig_ph_ali}\"\n",
    "\n",
    "    items = [test_ds[i] for i in range(30)]\n",
    "    batch = test_ds.collate_pad(items)\n",
    "    assert len(batch['feats'].shape) == 3, batch['feats'].shape\n",
    "    assert batch['feats'].shape[1] == 30, batch['feats'].shape\n",
    "    \n",
    "    assert len(batch['src_key_padding_mask'].shape) == 2, batch['src_key_padding_mask'].shape\n",
    "    assert batch['src_key_padding_mask'].shape[0] == 30, batch['src_key_padding_mask'].shape\n",
    "    assert batch['src_key_padding_mask'].shape[1] == batch['feats'].shape[0], f\"{batch['feats'].shape} {batch['src_key_padding_mask'].shape}\"\n",
    "    \n",
    "    number_nonmasked_frames = (~batch['src_key_padding_mask']).sum()\n",
    "    assert number_nonmasked_frames == len(batch['targets']), f\"{number_nonmasked_frames} != {len(batch['targets'])}\"\n",
    "\n",
    "    accumulated_len = 0\n",
    "    for i, item in enumerate(items):\n",
    "        feats = batch['feats'][:, i, :]\n",
    "        assert torch.isclose(feats.sum(), item['feats'].sum()) , i\n",
    "        src = batch['src_key_padding_mask'][i, :]\n",
    "        cutted_feats = feats[~src]\n",
    "        assert torch.isclose(item['feats'], cutted_feats).all()\n",
    "        cutted_targets = batch['targets'][accumulated_len: accumulated_len + cutted_feats.shape[0]]\n",
    "        assert torch.isclose(cutted_targets, item['targets']).all()\n",
    "        accumulated_len += cutted_feats.shape[0]\n",
    "    print(\"Test 1.c passed\")\n",
    "    \n",
    "test_collate_pad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59eb65-dbbb-4d11-aa26-b2812b04219e",
   "metadata": {},
   "source": [
    "# 2. Акустическая модель\n",
    "\n",
    "Обучим TransformerEncoder из torch решать задачу пофреймовой классификации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AModel(nn.Module):\n",
    "    def __init__(self, feats_dim, out_dim,  dim=128, num_layers=4, ff_dim=256, dropout=0.1, nhead=4, max_len=780):\n",
    "        super().__init__()\n",
    "        self.feats_dim = feats_dim\n",
    "        self.max_len=max_len\n",
    "        self.input_ff = nn.Linear(feats_dim, dim)\n",
    "        self.positional_encoding = nn.Embedding(max_len, dim)\n",
    "        layer = torch.nn.TransformerEncoderLayer(d_model=dim, \n",
    "                                                 nhead=nhead, \n",
    "                                                 dim_feedforward=ff_dim, \n",
    "                                                 dropout=dropout, \n",
    "                                                 batch_first=False)\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer=layer, num_layers=num_layers)\n",
    "        \n",
    "        self.head = nn.Linear(dim, out_dim)\n",
    "\n",
    "    def forward(self, feats, src_key_padding_mask=None, **kwargs):\n",
    "        #TODO \n",
    "        # реализуйте прямой проход модели.\n",
    "        # Фичи подаются на первый ff слой, \n",
    "        # к результату прибавляются позиционные эмбединги.\n",
    "        # Далее фреймы обрабатываются трансформером \n",
    "        # и финализируются с помощью головы\n",
    "        pass\n",
    "        \n",
    "        return logits # (Time, Batch, Phones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828458c8-e53f-4dcd-b092-00358036cd2a",
   "metadata": {},
   "source": [
    "## 3. Обучение модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ef5b0-4e9d-4543-9cfa-a449ea5cebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартный пайплайн обучения моделей в pytorch\n",
    "class Trainer(nn.Module):\n",
    "    def __init__(self, model, fe, phone2id, device='cuda', opt_cls=torch.optim.Adam, opt_kwargs={'lr':0.0001}):\n",
    "        super().__init__()\n",
    "        self.device=device\n",
    "        self.fe = fe\n",
    "        self.model = model.to(self.device)\n",
    "        self.phone2id = phone2id\n",
    "        self.id2phone = {i:ph for ph,i in phone2id.items()}\n",
    "        self.optimizer = opt_cls(self.model.parameters(), **opt_kwargs)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        print(f\"{self.model}. {self.device}\")\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        return super().to()\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch = self.batch_to_device(batch)\n",
    "        logits = self.model(**batch)\n",
    "        # TODO\n",
    "        # реализуйте подсчет loss функции  \n",
    "        pass\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def batch_to_device(self, batch):\n",
    "        return {k: v.to(self.device) for k, v in batch.items()}\n",
    "        \n",
    "    def train_one_epoch(self, train_dataloader):\n",
    "        \"\"\" Цикл обучения одной эпохи по всем данным\"\"\"\n",
    "        self.model.train()\n",
    "        pbar = tqdm(train_dataloader)\n",
    "        losses = []\n",
    "        for batch in pbar:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.forward(batch)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_description(f\"training loss {losses[-1]:.5f}\")\n",
    "        return losses\n",
    "\n",
    "    def score(self, valid_dataloader) -> List[float]:\n",
    "        \"\"\"Подсчет лосса на валидационной выборке\"\"\"\n",
    "        pbar = tqdm(valid_dataloader, desc=\"Scoring...\")\n",
    "        losses = []\n",
    "        # TODO \n",
    "        # реализуйте функцию, которая подсчитывает лосс на валидационной выборке \n",
    "        # losses должен хранить значение ошибки на каждом батче \n",
    "        pass \n",
    "        \n",
    "        return losses\n",
    "\n",
    "    def fit(self, train_dataloader, epochs, valid_dataloader=None, plot_losses=True):\n",
    "        \"\"\"Запуск обучения на данном dataloader\"\"\"\n",
    "        pbar = tqdm(range(epochs))\n",
    "        per_epoch_train_losses = []\n",
    "        per_epoch_val_losses = []\n",
    "        for e in pbar:\n",
    "            train_loss = np.mean(self.train_one_epoch(train_dataloader))\n",
    "            per_epoch_train_losses.append(train_loss)\n",
    "            if valid_dataloader is not None:\n",
    "                val_loss = np.mean(self.score(valid_dataloader))\n",
    "                per_epoch_val_losses.append(val_loss)\n",
    "            if plot_losses:\n",
    "                display.clear_output()\n",
    "                self.plot_losses(per_epoch_train_losses, per_epoch_val_losses)\n",
    "            else:\n",
    "                val_loss = val_loss if valid_dataloader is not None else float('Nan')\n",
    "                print(f\"train: {train_loss:.5f} | val: {val_loss:.5f}\")\n",
    "        return per_epoch_train_losses, per_epoch_val_losses\n",
    "    \n",
    "    def plot_losses(self, train_losses, val_losses=[]):\n",
    "        plt.title(f\"Train test losses (epoch {len(train_losses)})\")\n",
    "        plt.plot(range(len(train_losses)), train_losses)\n",
    "        if len(val_losses)>0:\n",
    "            assert len(train_losses) == len(val_losses)\n",
    "            plt.plot(range(len(val_losses)), val_losses)\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend([\"train loss\", \"valid loss\"])\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "                 \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8be5c-c1dd-4587-ac67-b74d27f1e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfit_one_batch_check():\n",
    "    # Для проверки работоспособности кода обучения удоно использовать тест модели на overfit \n",
    "    # Для этого запускается обучение на одном батче данных. \n",
    "    # Если код написан правильно, то модель обязана выучить выучить все примеры из этого батча наизусть. \n",
    "    fe = FeatureExtractor()\n",
    "    train_dataset = FeatsPhoneDataset('timit/data/TEST/DR1/FAKS0', feature_extractor=fe, phone2id=PHONE2ID)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, collate_fn=train_dataset.collate_pad)\n",
    "    test_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, collate_fn=train_dataset.collate_pad)\n",
    "\n",
    "    trainer = Trainer(model=AModel(feats_dim=fe.feats_dim, \n",
    "                                   out_dim=max(PHONE2ID.values()) + 1,  \n",
    "                                   dim=256, \n",
    "                                   num_layers=6, \n",
    "                                   ff_dim=512, \n",
    "                                   dropout=0.0, \n",
    "                                   nhead=8), \n",
    "                      fe=fe, \n",
    "                      phone2id=PHONE2ID, device='cuda')\n",
    "   \n",
    "    # only one batch. The model must learn it by heart\n",
    "    losses, val_losses = trainer.fit(train_dataloader, 160, valid_dataloader=test_dataloader, plot_losses=False)\n",
    "\n",
    "    trainer.plot_losses(losses, val_losses)\n",
    "\n",
    "    val_loss = np.mean(trainer.score(test_dataloader))\n",
    "    \n",
    "    assert val_loss < 0.5, f\"{val_loss}. Model doesn't train well\" \n",
    "    print(f\"Test 3.a passed\")\n",
    "overfit_one_batch_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e661a5c9-f31f-4dba-a6cf-1a8ada61d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def experiment():\n",
    "    # Запуск полноценного обучения модели\n",
    "    # TODO: Тюнинг гиперпараметров\n",
    "    fe = FeatureExtractor()\n",
    "    test_dataset = FeatsPhoneDataset('timit/data/TEST/', feature_extractor=fe, phone2id=PHONE2ID)\n",
    "    train_dataset = FeatsPhoneDataset('timit/data/TRAIN/', feature_extractor=fe, phone2id=PHONE2ID)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=40, \n",
    "                                               num_workers=0, collate_fn=train_dataset.collate_pad, shuffle=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, \n",
    "                                               num_workers=0, collate_fn=test_dataset.collate_pad, shuffle=False)\n",
    "\n",
    "\n",
    "    trainer = Trainer(model=AModel(feats_dim=fe.feats_dim, \n",
    "                                 out_dim=max(PHONE2ID.values())+1, \n",
    "                                 dim=128, \n",
    "                                 num_layers=7, \n",
    "                                 ff_dim=256, \n",
    "                                 dropout=0.0, \n",
    "                                 nhead=8),\n",
    "                     fe=fe, \n",
    "                     phone2id=PHONE2ID, device='cuda')\n",
    "\n",
    "    trainer.fit(train_dataloader, epochs=40, valid_dataloader=test_dataloader, plot_losses=True)\n",
    "    return trainer.to('cpu')\n",
    "results = experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82fb7a-38f9-4618-9030-aabdd62a3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(results, 'baseline.trainer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df28a9-24e8-405a-ab07-7acef7036a13",
   "metadata": {},
   "source": [
    "# Основное задание (12 баллов)\n",
    "Надо улучшить бейзлайн так, чтобы значение loss на валидации было менее 1.9 \n",
    "\n",
    "**Дополнительное задание** (4 балла): Улучшите loss до 1.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0aa4fd-fb40-43f3-915b-e8f2a447fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_model(trainer):\n",
    "    test_dataset = FeatsPhoneDataset('timit/data/TEST/', feature_extractor=trainer.fe, phone2id=trainer.phone2id)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, \n",
    "                                               num_workers=0, collate_fn=test_dataset.collate_pad, shuffle=False)\n",
    "    loss = np.mean(trainer.score(test_dataloader))\n",
    "    print(f\"Test loss is {loss}\")\n",
    "    assert loss < 1.8, \"Main task failed\"\n",
    "    print(f\"Main task is done! (12 points)\")\n",
    "    if loss <= 1.3:\n",
    "        print(f\"Additional task is done! (+4 points)\")\n",
    "test_trained_model(results.to('cuda'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "04d0eabe",
      "metadata": {
        "id": "04d0eabe"
      },
      "source": [
        "# Языковые модели\n",
        "\n",
        "Языковые модели играют важную роль в системах распознавания речи, помогая создавать более грамотные и лексически корректные тексты. В данной работе мы будем изучать нграмные языковые модели, которые позволяют довольно легко оценить вероятность и правдоподобность текста.\n",
        "\n",
        "В нграмной языковой модели, нграм - это последовательность из n слов в тексте. Например, в предложении \"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\", биграмами будут \"по-моему мы\", \"мы сэкономим\", \"сэкономим уйму\" итд. Языковые модели оценивают вероятность появления последовательности слов, исходя из статистики появления каждого из нграм в обучающей выборке.\n",
        "\n",
        "Порядком (order) нграм языковой модели называют максимальную длину нграм, которую учитывает модель.\n",
        "\n",
        "Практическая работа разделена на 2 части:\n",
        "1. Построение нграмой языковой модели - основная часть, 10 баллов\n",
        "1. Предсказание с помощью языковой модели - дополнительная часть, 6 балла\n",
        "\n",
        "\n",
        "\n",
        "Полезные сслыки:\n",
        "* arpa формат - https://cmusphinx.github.io/wiki/arpaformat/\n",
        "* обучающие материалы - https://pages.ucsd.edu/~rlevy/teaching/2015winter/lign165/lectures/lecture13/lecture13_ngrams_with_SRILM.pdf\n",
        "* обучающие материалы.2 - https://cjlise.github.io/machine-learning/N-Gram-Language-Model/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "4bd5c324",
      "metadata": {
        "id": "4bd5c324"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0c1c1d7",
      "metadata": {
        "id": "b0c1c1d7"
      },
      "source": [
        "# 1. Построение нграмной языковой модели. (10 баллов)\n",
        "\n",
        "\n",
        "Вероятность текста с помощью нграмной языковой модели можно вычислить по формуле:\n",
        "$$ P(w_1, w_2, .., w_n) = {\\prod{{P_{i=0}^{n}(w_i| w_{i-order}, .., w_{i-1})}}} $$\n",
        "\n",
        "В простом виде, при обучении нграмной языковой модели, чтобы рассчитать условную вероятность каждой нграмы, используется формула, основанная на количестве появлений нграмы в обучающей выборке. Формула выглядит следующим образом:\n",
        "$$ P(w_i| w_{i-order}, .., w_{i-1}) = {{count(w_{i-order}, .., w_{i})} \\over {count(w_{i-order},..., w_{i-1})}} $$\n",
        "\n",
        "Поскольку униграмы не содержат в себе какого-дибо контекста, вероятность униграмы можно посчитать поделив кол-во этой слова на общее количество слов в обучающей выборке.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "5837fe90",
      "metadata": {
        "id": "5837fe90"
      },
      "outputs": [],
      "source": [
        "# в первую очередь нам понадобится подсчитать статистику по обучающей выборке\n",
        "def count_ngrams(train_text: List[str], order=3, bos=True, eos=True) -> Dict[Tuple[str], int]:\n",
        "    ngrams = defaultdict(int)\n",
        "    # TODO реализуйте функцию, которая подсчитывает все 1-gram 2-gram ... order-gram ngram'ы в тексте\n",
        "    for sentence in train_text:\n",
        "      tokens = sentence.split()\n",
        "      # добавляем символ начала предложения (<s>) в начало, если bos=True\n",
        "      if bos:\n",
        "        tokens = ['<s>'] + tokens\n",
        "      # добавляем символ конца предложения (</s>) в конец, если eos=True\n",
        "      if eos:\n",
        "        tokens = tokens + ['</s>']\n",
        "      for i in range(1, order+1):\n",
        "        for j in range(len(tokens) - i + 1):\n",
        "                ngrams[tuple(tokens[j:j+i])] += 1\n",
        "\n",
        "\n",
        "    #\n",
        "    return dict(ngrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "fd69d44d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd69d44d",
        "outputId": "d3c84e6c-1624-4fe7-baac-31ae8a8b0d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1a passed\n"
          ]
        }
      ],
      "source": [
        "def test_count_ngrams():\n",
        "    assert count_ngrams(['привет привет как дела'], order=1, bos=True, eos=True) == {\n",
        "        ('<s>',): 1,\n",
        "        ('привет',): 2,\n",
        "        ('как',): 1,\n",
        "        ('дела',): 1,\n",
        "        ('</s>',): 1\n",
        "    }\n",
        "    assert count_ngrams(['привет привет как дела'], order=1, bos=False, eos=True) == {\n",
        "        ('привет',): 2,\n",
        "        ('как',): 1,\n",
        "        ('дела',): 1,\n",
        "        ('</s>',): 1\n",
        "    }\n",
        "    assert count_ngrams(['привет привет как дела'], order=1, bos=False, eos=False) == {\n",
        "        ('привет',): 2,\n",
        "        ('как',): 1,\n",
        "        ('дела',): 1\n",
        "    }\n",
        "    assert count_ngrams(['привет привет как дела'], order=2, bos=False, eos=False) == {\n",
        "        ('привет',): 2,\n",
        "        ('как',): 1,\n",
        "        ('дела',): 1,\n",
        "        ('привет', 'привет'): 1,\n",
        "        ('привет', 'как'): 1,\n",
        "        ('как', 'дела'): 1\n",
        "    }\n",
        "    assert count_ngrams(['привет ' * 6], order=2, bos=False, eos=False) == {\n",
        "        ('привет',): 6,\n",
        "        ('привет', 'привет'): 5\n",
        "    }\n",
        "    result = count_ngrams(['практическое сентября',\n",
        "                           'второе практическое занятие пройдет в офлайне 32 сентября в 12 часов 32 минуты',\n",
        "                           'в офлайне в 32 12'], order=5)\n",
        "    assert result[('<s>',)] == 3\n",
        "    assert result[('32',)] == 3\n",
        "    assert result[('<s>', 'в', 'офлайне', 'в', '32')] == 1\n",
        "    assert result[('офлайне', 'в', '32', '12', '</s>')] == 1\n",
        "    print('Test 1a passed')\n",
        "\n",
        "\n",
        "test_count_ngrams()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac6e1865",
      "metadata": {
        "id": "ac6e1865"
      },
      "source": [
        "\n",
        "Простой подход к вычислению вероятностей через количество нграм имеет существенный недостаток. Если в тексте встретится нграмма, которой не было в обучающей выборке, то вероятность всего текста будет равна нулю.\n",
        "\n",
        "Чтобы избежать данного недостатка, вводится специальное сглаживание - add-k сглаживание ([Additive, Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)). Данная техника позволяет учитывать нграмы, не встретившиеся в обучающей выборке, и при этом не делает вероятность текста равной нулю.\n",
        "\n",
        "Формула сглаживания Лапласа выглядит следующим образом:\n",
        "\n",
        "$$ P(w_i| w_{i-order}, .., w_{i-1}) = {{count(w_{i-order}, .., w_{i}) + k} \\over {count(w_{i-order},..., w_{i-1}) + k*V}} $$\n",
        "\n",
        "Здесь V - количество слов в словаре, а k - гиперпараметр, который контролирует меру сглаживания. Как правило, значение k выбирается экспериментально, чтобы найти оптимальный баланс между учетом редких нграм и сохранением вероятности для часто встречающихся нграм.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "4cafb4b8",
      "metadata": {
        "id": "4cafb4b8"
      },
      "outputs": [],
      "source": [
        "# функция подсчета вероятности через количество со сглаживанием Лапласа\n",
        "def calculate_ngram_prob(ngram: Tuple[str], counts: Dict[Tuple[str], int], V=None, k=0) -> float:\n",
        "    # подсчитывет ngram со сглаживанием Лапласа\n",
        "    # TODO\n",
        "    # если размер словаря не указан, рассчитываем его\n",
        "    if V is None:\n",
        "        V = sum(1 for key in counts if len(key) == 1)\n",
        "    # считаем количество предшественников n-граммы.\n",
        "    prefix_count = sum(value for key, value in counts.items()\n",
        "                       if len(key) == len(ngram) and key[:-1] == ngram[:-1])\n",
        "    # получаем частоту n-граммы из словаря\n",
        "    count = counts.get(ngram, 0)\n",
        "     # вычисляем вероятность n-граммы по формуле сглаживания Лапласа\n",
        "    prob = (count + k) / (prefix_count + k * V)\n",
        "\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "60b25d7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60b25d7f",
        "outputId": "6e363001-067f-4606-cc40-c4e7e4bb75b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1.b passed\n"
          ]
        }
      ],
      "source": [
        "def test_calculate_ngram_prob():\n",
        "    counts = count_ngrams(['практическое сентября',\n",
        "                           'второе практическое занятие в офлайне 32 сентября в 12 часов 32 минуты',\n",
        "                           'в офлайне в 32 12'], order=4)\n",
        "    assert calculate_ngram_prob(('в', 'офлайне'), counts) == 0.5\n",
        "    assert calculate_ngram_prob(('в', ), counts) == 4/25\n",
        "    assert calculate_ngram_prob(('в', ), counts, k=0.5) == (4+0.5)/(25+0.5*12)\n",
        "    assert calculate_ngram_prob(('в', 'офлайне', 'в', '32'), counts) == 1.0\n",
        "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=1) == 0.1875\n",
        "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=0.5) == 0.25\n",
        "    assert calculate_ngram_prob(('в', 'онлайне'), counts, k=0) == 0.0\n",
        "    assert calculate_ngram_prob(('в', 'онлайне'), counts, k=1) == 0.0625\n",
        "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=0.5) == 0.25\n",
        "\n",
        "    print(\"Test 1.b passed\")\n",
        "\n",
        "\n",
        "test_calculate_ngram_prob()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da494bf0",
      "metadata": {
        "id": "da494bf0"
      },
      "source": [
        "Основной метрикой язковых моделей является перплексия.\n",
        "\n",
        "Перплексия  — безразмерная величина, мера того, насколько хорошо распределение вероятностей предсказывает выборку. Низкий показатель перплексии указывает на то, что распределение вероятности хорошо предсказывает выборку.\n",
        "\n",
        "$$ ppl = {P(w_1, w_2 ,..., w_N)^{- {1} \\over {N}}} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "id": "4bd1f2e5",
      "metadata": {
        "id": "4bd1f2e5"
      },
      "outputs": [],
      "source": [
        "# Языковая модель\n",
        "class NgramLM:\n",
        "    def __init__(self, order=3, bos=True, eos=True, k=1, predefined_vocab=None):\n",
        "        self.order = order\n",
        "        self.eos = eos\n",
        "        self.bos = bos\n",
        "        self.k = k\n",
        "        self.vocab = predefined_vocab\n",
        "        self.ngrams_count = None\n",
        "\n",
        "    @property\n",
        "    def V(self) -> int:\n",
        "        return len(self.vocab)\n",
        "    #добавляем уже готовые функции из заданий выше\n",
        "    def _count_ngrams(self, train_text: List[str]) -> Dict[Tuple[str], int]:\n",
        "      ngrams = defaultdict(int)\n",
        "      for sentence in train_text:\n",
        "            tokens = sentence.split()\n",
        "            if self.bos:\n",
        "                tokens = ['<s>'] + tokens\n",
        "            if self.eos:\n",
        "                tokens = tokens + ['</s>']\n",
        "\n",
        "            for i in range(1, self.order + 1):\n",
        "                for j in range(len(tokens) - i + 1):\n",
        "                    ngrams[tuple(tokens[j:j+i])] += 1\n",
        "      return dict(ngrams)\n",
        "\n",
        "    def _calculate_ngram_prob(self, ngram: Tuple[str]) -> float:\n",
        "      #if V is None:\n",
        "          #V = sum(1 for key in self.ngrams_count if len(key) == 1)\n",
        "\n",
        "          prefix_count = sum(value for key, value in self.ngrams_count.items()\n",
        "                            if len(key) == len(ngram) and key[:-1] == ngram[:-1])\n",
        "\n",
        "          count = self.ngrams_count.get(ngram, 0)\n",
        "          prob = (count + self.k) / (prefix_count + self.k * self.V)\n",
        "\n",
        "          return prob\n",
        "\n",
        "    def fit(self, train_text: List[str]) -> None:\n",
        "        # TODO\n",
        "        # Подсчет vocab и ngrams_count по обучающей выборке\n",
        "        self.ngrams_count = self._count_ngrams(train_text)\n",
        "        self.vocab = {ngram[0] for ngram in self.ngrams_count.keys() if len(ngram) == 1}\n",
        "\n",
        "\n",
        "    def predict_ngram_log_proba(self, ngram: Tuple[str]) -> float:\n",
        "        # TODO\n",
        "        # считаем логарифм вероятности конкретной нграмы\n",
        "        return np.log(self._calculate_ngram_prob(ngram))\n",
        "\n",
        "\n",
        "    def predict_log_proba(self, words: List[str]) -> float:\n",
        "        if self.bos:\n",
        "            words = ['<s>'] + words\n",
        "        if self.eos:\n",
        "            words = words + ['</s>']\n",
        "        logprob = 0\n",
        "        # TODO\n",
        "        # применяем chain rule, чтобы посчитать логарифм вероятности всей строки\n",
        "        logprob = sum(self.predict_ngram_log_proba(tuple(words[max(0, i - self.order + 1):i + 1]))\n",
        "                                                    for i in range(len(words)))\n",
        "\n",
        "        return logprob\n",
        "\n",
        "    def ppl(self, text: List[str]) -> float:\n",
        "        #TODO\n",
        "        # подсчет перплексии\n",
        "        # Для того, чтобы ваш код был численно стабильным,\n",
        "        #    не считайте формулу напрямую, а воспользуйтесь переходом к логарифмам вероятностей\n",
        "        # сумма логарифмов вероятностей слов в тексте\n",
        "        total_logprob = sum(self.predict_log_proba(line.split()) for line in text)\n",
        "        # общее количество слов в тексте, включая начальные и конечные токены\n",
        "        total_words = sum(len(line.split()) for line in text) + (self.bos + self.eos) * len(text)\n",
        "        # вычисляем перплексию как экспоненту от отрицательного среднего логарифма вероятности\n",
        "        # если общее количество слов равно 0, перплексия устанавливается в бесконечность\n",
        "        return np.exp(-total_logprob / total_words) if total_words > 0 else float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "id": "bb0bfe64",
      "metadata": {
        "id": "bb0bfe64"
      },
      "outputs": [],
      "source": [
        "def test_lm():\n",
        "    train_data = [\"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\",\n",
        "                  \"если я сойду с ума прямо сейчас по-моему мы сэкономим уйму времени\",\n",
        "                  \"мы сэкономим уйму времени если я сейчас сойду с ума по-моему\"]\n",
        "    global lm\n",
        "    lm = NgramLM(order=2)\n",
        "    lm.fit(train_data)\n",
        "    assert lm.V == 14\n",
        "    assert np.isclose(lm.predict_log_proba(['мы']), lm.predict_log_proba([\"если\"]))\n",
        "    assert lm.predict_log_proba([\"по-моему\"]) > lm.predict_log_proba([\"если\"])\n",
        "\n",
        "    gt = ((3+1)/(41 + 14) * 1/(3+14))**(-1/2)\n",
        "    ppl = lm.ppl([''])\n",
        "    assert  np.isclose(ppl, gt), f\"{ppl=} {gt=}\"\n",
        "\n",
        "    gt = ((3+1)/(41 + 14) * 1/(3+14) * 1/(14)) ** (-1/3)\n",
        "    ppl = lm.ppl(['ЧТО'])\n",
        "    assert  np.isclose(ppl, gt), f\"{ppl=} {gt=}\"\n",
        "\n",
        "    test_data = [\"по-моему если я прямо сейчас сойду с ума мы сэкономим уйму времени\"]\n",
        "    ppl = lm.ppl(test_data)\n",
        "    assert round(ppl, 2) == 7.33, f\"{ppl}\"\n",
        "test_lm()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edafa0a2",
      "metadata": {
        "id": "edafa0a2"
      },
      "source": [
        "# 2. Предсказания с помощью языковой модели (6 балла)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "id": "85d2eb63",
      "metadata": {
        "id": "85d2eb63"
      },
      "outputs": [],
      "source": [
        "def predict_next_word(lm: NgramLM, prefix: List[str], topk=4):\n",
        "    # TODO реализуйте функцию, которая предсказывает продолжение фразы.\n",
        "    # верните topk наиболее вероятных продолжений фразы prefix\n",
        "\n",
        "    # Предсказание следующих слов на основе префикса\n",
        "    top_prob = sorted(\n",
        "        # получаем логарифмические вероятности для каждого слова из словаря после префикса\n",
        "        ((word, lm.predict_ngram_log_proba(tuple(prefix + [word])))\n",
        "         for word in lm.vocab),\n",
        "        # сортируем по убыванию логарифмической вероятности\n",
        "        key=lambda e: e[1],\n",
        "        reverse=True\n",
        "    )[:topk]\n",
        "\n",
        "    return [(word, log_prob) for word, log_prob in top_prob]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fc4846b",
      "metadata": {
        "id": "3fc4846b"
      },
      "source": [
        "Попробуйте обучить ngram языковую модель на нескольких стихотворениях. Не забудьте трансформировать стихотворение в удобный для ngram модели формат (как сделать так, чтобы модель моделировала рифму?).\n",
        "Попробуйте сгенерировать продолжение для стихотворения с помощью такой языковой модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "id": "107862fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "107862fe",
        "outputId": "01366473-f1d6-4c55-cd6e-05ce9a5ad1eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('умирать', -3.6375861597263857),\n",
              " ('меня', -3.6375861597263857),\n",
              " ('мой,', -3.6375861597263857),\n",
              " ('жить,', -3.6375861597263857)]"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ],
      "source": [
        "poem = [ \"\"\"\n",
        "До свиданья, друг мой, до свиданья.\n",
        "Милый мой, ты у меня в груди.\n",
        "Предназначенное расставанье\n",
        "Обещает встречу впереди.\n",
        "До свиданья, друг мой, без руки, без слова,\n",
        "Не грусти и не печаль бровей, —\n",
        "В этой жизни умирать не ново,\n",
        "Но и жить, конечно, не новей.\n",
        "\"\"\"]\n",
        "lm = NgramLM(order=3)\n",
        "lm.fit(poem)\n",
        "\n",
        "predict_next_word(lm, [\"мой\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = \"\"\"\n",
        "У лукоморья дуб зелёный;\n",
        "Златая цепь на дубе том:\n",
        "И днём и ночью кот учёный\n",
        "Всё ходит по цепи кругом;\n",
        "Идёт направо — песнь заводит,\n",
        "Налево — сказку говорит.\n",
        "Там чудеса: там леший бродит,\n",
        "Русалка на ветвях сидит;\n",
        "Там на неведомых дорожках\n",
        "Следы невиданных зверей;\n",
        "Избушка там на курьих ножках\n",
        "Стоит без окон, без дверей;\n",
        "Там лес и дол видений полны;\n",
        "Там о заре прихлынут волны\n",
        "На брег песчаный и пустой,\n",
        "И тридцать витязей прекрасных\n",
        "Чредой из вод выходят ясных,\n",
        "И с ними дядька их морской;\n",
        "Там королевич мимоходом\n",
        "Пленяет грозного царя;\n",
        "Там в облаках перед народом\n",
        "Через леса, через моря\n",
        "Колдун несёт богатыря;\n",
        "В темнице там царевна тужит,\n",
        "А бурый волк ей верно служит;\n",
        "Там ступа с Бабою Ягой\n",
        "Идёт, бредёт сама собой,\n",
        "Там царь Кащей над златом чахнет;\n",
        "Там русский дух… там Русью пахнет!\n",
        "И там я был, и мёд я пил;\n",
        "У моря видел дуб зелёный;\n",
        "Под ним сидел, и кот учёный\n",
        "Свои мне сказки говорил.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DsGInbliiTqg"
      },
      "id": "DsGInbliiTqg",
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "xAACY4jBitV9"
      },
      "id": "xAACY4jBitV9",
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized = train.strip().lower()\n",
        "normalized = re.sub(r\"\\s+\", \" \", normalized)\n",
        "normalized = re.sub(r\"^\\s*\\$\\$.+\\$\\$\\s*$\", \"\", normalized, flags=re.MULTILINE)\n",
        "normalized = re.sub(r\"\\s+-\\s+\", \" \", normalized)\n",
        "normalized = re.sub(r\"[^\\w\\d\\.\\?!\\-\\s]\", \" \", normalized)"
      ],
      "metadata": {
        "id": "lag8mv_2fYAa"
      },
      "id": "lag8mv_2fYAa",
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = re.split(r\"[\\.!?]+|\\n+\", normalized)\n",
        "sentences = [sentence.strip() for sentence in sentences if sentence.strip() != \"\"]"
      ],
      "metadata": {
        "id": "I8SdT6d-lekB"
      },
      "id": "I8SdT6d-lekB",
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = sentences\n",
        "\n",
        "lm = NgramLM(order=3)\n",
        "lm.fit(data)"
      ],
      "metadata": {
        "id": "5WSo_43_jBMJ"
      },
      "id": "5WSo_43_jBMJ",
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"дуб\"\n",
        "length = 6\n",
        "\n",
        "# инициализация текста начальным словом\n",
        "text = word\n",
        "for _ in range(length):\n",
        "    # получение последнего слова из текущего текста\n",
        "    last_word = text.split()[-1]\n",
        "    # предсказание следующего слова\n",
        "    next_word = predict_next_word(lm, [last_word], topk=1)[0][0]\n",
        "    # добавление предсказанного слова к тексту\n",
        "    text += f\" {next_word}\"\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ueaV7CkNKZ2Z",
        "outputId": "b661ae63-d8b7-4aaf-be74-ef0ab0d4c8eb"
      },
      "id": "ueaV7CkNKZ2Z",
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'дуб зелёный златая цепь на курьих ножках'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}